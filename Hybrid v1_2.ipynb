{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nzdXrtNkinsN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\argon\\anaconda3\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras-tuner) (3.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras-tuner) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (3.12.1)\n",
      "Requirement already satisfied: optree in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\argon\\anaconda3\\lib\\site-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from requests->keras-tuner) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from rich->keras->keras-tuner) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from rich->keras->keras-tuner) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\argon\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras->keras-tuner) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jzvLpVFkizJP"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 1) Custom Layers for Indices\n",
    "##############################################################################\n",
    "class PatchIndex(tf.keras.layers.Layer):\n",
    "    \"\"\"Returns (batch, num_patches) with values [0..num_patches-1].\"\"\"\n",
    "    def __init__(self, num_patches, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, x):\n",
    "        bs = tf.shape(x)[0]\n",
    "        idx = tf.range(self.num_patches)\n",
    "        idx = tf.expand_dims(idx, 0)  # shape (1, num_patches)\n",
    "        idx = tf.tile(idx, [bs, 1])   # shape (batch, num_patches)\n",
    "        return idx\n",
    "\n",
    "class ClassTokenIndex(tf.keras.layers.Layer):\n",
    "    \"\"\"Returns shape (batch,1) all zeros for the class token embedding.\"\"\"\n",
    "    def call(self, x):\n",
    "        bs = tf.shape(x)[0]\n",
    "        idx = tf.range(1)        # [0]\n",
    "        idx = tf.expand_dims(idx, 0)  # (1,1)\n",
    "        idx = tf.tile(idx, [bs,1])    # (batch,1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lPMwJTUvi4-A"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 2) Warmup + CosineDecay Learning Rate\n",
    "##############################################################################\n",
    "def create_warmup_cosine_lr(initial_lr, warmup_steps, total_steps):\n",
    "    \"\"\"\n",
    "    Composite schedule:\n",
    "      - Warmup from 0 -> initial_lr over `warmup_steps`\n",
    "      - Cosine decay from initial_lr -> 0 over `(total_steps - warmup_steps)`\n",
    "    \"\"\"\n",
    "    def lr_fn(step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        if step < warmup_steps:\n",
    "            return initial_lr * (step / tf.cast(warmup_steps, tf.float32))\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return 0.5 * initial_lr * (1.0 + tf.cos(np.pi * progress))\n",
    "\n",
    "    return tf.keras.optimizers.schedules.LearningRateSchedule(lr_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KUW4gq0Ji5yB"
   },
   "outputs": [],
   "source": [
    "class WarmupCosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    1) Warmup from 0 to initial_lr over warmup_steps\n",
    "    2) Cosine decay from initial_lr down to 0 over (total_steps - warmup_steps).\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_lr, warmup_steps, total_steps):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.warmup_steps = float(warmup_steps)\n",
    "        self.total_steps = float(total_steps)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "\n",
    "        # Warmup phase\n",
    "        def warmup_fn():\n",
    "            return self.initial_lr * (step / self.warmup_steps)\n",
    "\n",
    "        # Cosine decay phase\n",
    "        def cosine_fn():\n",
    "            progress = (step - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            return 0.5 * self.initial_lr * (1.0 + tf.cos(np.pi * progress))\n",
    "\n",
    "        return tf.cond(step < self.warmup_steps, lambda: warmup_fn(), lambda: cosine_fn())\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'initial_lr': self.initial_lr,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'total_steps': self.total_steps\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ny8c0JqZi8J7"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 3) Hybrid CNN-Transformer with More Depth & LR Schedule\n",
    "##############################################################################\n",
    "def build_deeper_hybrid(hp):\n",
    "    \"\"\"\n",
    "    Key improvements:\n",
    "      - Up to 3 Conv blocks in the CNN\n",
    "      - Up to 4 Transformer blocks\n",
    "      - hidden_dim up to 512\n",
    "      - warmup + cosine LR schedule\n",
    "    \"\"\"\n",
    "    n, m = 6, 7\n",
    "    num_patches = n * m\n",
    "    num_classes = 7\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_conv_blocks = hp.Int('num_conv_blocks', min_value=2, max_value=3)\n",
    "    embed_dim       = hp.Choice('embed_dim', [32, 64, 128])\n",
    "    hidden_dim      = hp.Choice('hidden_dim', [128, 256, 512])\n",
    "    num_layers      = hp.Int('num_layers', min_value=2, max_value=4)\n",
    "    num_heads       = hp.Choice('num_heads', [2, 4, 8])\n",
    "    dropout_rate    = hp.Choice('dropout_rate', [0.0, 0.1, 0.2])\n",
    "    base_lr         = hp.Choice('base_lr', [1e-4, 3e-4, 1e-3])\n",
    "\n",
    "    key_dim   = hidden_dim // num_heads\n",
    "    value_dim = key_dim * 2\n",
    "    mlp_dim   = hidden_dim\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(n, m, 2))\n",
    "\n",
    "    # 1) CNN Embedding\n",
    "    x = inp\n",
    "    for i in range(num_conv_blocks):\n",
    "        filters = embed_dim * (i + 1)\n",
    "        x = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', activation='relu')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(embed_dim, kernel_size=1, activation='relu')(x)\n",
    "    x = tf.keras.layers.Reshape((num_patches, embed_dim))(x)\n",
    "\n",
    "    # 2) Transformer Layers\n",
    "    x = tf.keras.layers.Dense(hidden_dim)(x)\n",
    "    patch_idx = PatchIndex(num_patches)(x)\n",
    "    pos_emb = tf.keras.layers.Embedding(input_dim=num_patches, output_dim=hidden_dim)(patch_idx)\n",
    "    x = tf.keras.layers.Add()([x, pos_emb])\n",
    "\n",
    "    cls_idx = ClassTokenIndex()(x)\n",
    "    cls_token = tf.keras.layers.Embedding(input_dim=1, output_dim=hidden_dim)(cls_idx)\n",
    "    x = tf.keras.layers.Concatenate(axis=1)([cls_token, x])\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        ln1 = tf.keras.layers.LayerNormalization()(x)\n",
    "        attn_out = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=key_dim, value_dim=value_dim, dropout=dropout_rate\n",
    "        )(ln1, ln1, ln1)\n",
    "        x = tf.keras.layers.Add()([x, attn_out])\n",
    "\n",
    "    cls_out = x[:, 0, :]\n",
    "    cls_out = tf.keras.layers.LayerNormalization()(cls_out)\n",
    "    logits  = tf.keras.layers.Dense(num_classes, activation='softmax')(cls_out)\n",
    "\n",
    "    model = tf.keras.models.Model(inp, logits)\n",
    "\n",
    "    # 3) LR Schedule\n",
    "    warmup_steps = 2000\n",
    "    total_steps  = 20000\n",
    "    lr_schedule  = lr_schedule = WarmupCosineSchedule(base_lr, warmup_steps, total_steps)\n",
    "\n",
    "    optimizer    = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xloaemcai9qH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\argon\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |3                 |num_conv_blocks\n",
      "128               |128               |embed_dim\n",
      "512               |512               |hidden_dim\n",
      "3                 |3                 |num_layers\n",
      "4                 |4                 |num_heads\n",
      "0                 |0                 |dropout_rate\n",
      "0.0001            |0.0001            |base_lr\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# 4) Tuning & Training Example\n",
    "##############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    #shutil.rmtree('deeper_hybrid_tuner', ignore_errors=True)  # Clear old tuner results\n",
    "\n",
    "    full_comb_X = np.load(\"C:/Users/argon/Documents/Desktop Prime/MS Business Analytics/Spring Semester/Optimization - II/Connect 4 Project/final_deduplicated_X.npy\")\n",
    "    full_comb_Y = np.load(\"C:/Users/argon/Documents/Desktop Prime/MS Business Analytics/Spring Semester/Optimization - II/Connect 4 Project/final_deduplicated_Y.npy\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(full_comb_X, full_comb_Y, test_size=0.15, shuffle=True, random_state=42)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        build_deeper_hybrid,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=10,\n",
    "        factor=3,\n",
    "        directory='deeper_hybrid_tuner',\n",
    "        project_name='cnn_transformer_improved'\n",
    "    )\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=128, callbacks=[stop_early], shuffle=True)\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "    # âœ… Print all available hyperparameters before accessing them\n",
    "    print(\"\\nAvailable hyperparameters:\", best_hps.values)\n",
    "\n",
    "    # âœ… Get hyperparameters safely\n",
    "    print(\"\\nBest Hyperparams found by Hyperband:\")\n",
    "    print(\" - num_conv_blocks=\", best_hps.get('num_conv_blocks'))\n",
    "    print(\" - embed_dim     =\", best_hps.get('embed_dim'))\n",
    "    print(\" - hidden_dim    =\", best_hps.get('hidden_dim'))\n",
    "    print(\" - num_layers    =\", best_hps.get('num_layers'))\n",
    "    print(\" - num_heads     =\", best_hps.get('num_heads'))\n",
    "    print(\" - dropout_rate  =\", best_hps.get('dropout_rate'))\n",
    "    print(\" - base_lr       =\", best_hps.get('base_lr'))\n",
    "\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    ckpt_path = '/content/best_hybrid_model_fixed_2.keras'\n",
    "    ckpt_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        ckpt_path, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "\n",
    "    final_history = best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=60, batch_size=128, callbacks=[stop_early, ckpt_cb], shuffle=True)\n",
    "\n",
    "    val_loss, val_acc = best_model.evaluate(X_val, y_val)\n",
    "    print(f\"\\nFinal validation accuracy: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
